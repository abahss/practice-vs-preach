{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc388fa0-3a8b-4840-848b-9f15fc7d8cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "import string # \"string\" module is already installed with Python\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "import time\n",
    "from requests.exceptions import ChunkedEncodingError, ConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70230960-9594-40e5-8bbb-fae07b2e4956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUBLIC key, so no need to hide\n",
    "key = 'OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5bc99-532e-4c23-a97a-bca4e1cc88ae",
   "metadata": {},
   "source": [
    "### Retrieve parliamentary speech minutes (API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557d179-6fd3-4c9c-9a91-48dfb41715ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep=22 #define latest election period of interest\n",
    "earliest_ep=20 #define earliest election period of interest\n",
    "responses = []\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb321786",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_attempts = 3\n",
    "while ep > earliest_ep:\n",
    "    url = f\"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text?f.zuordnung=BT&f.datum.end={end_date}&apikey={key}\"\n",
    "\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            payload = response.json()\n",
    "            break\n",
    "        except (ChunkedEncodingError, ConnectionError) as exc:\n",
    "            print(f\"Attempt {attempt}/{max_attempts} failed: {exc}\")\n",
    "            if attempt == max_attempts:\n",
    "                print(\"Giving up on this request, continuing with next loop iteration.\")\n",
    "                payload = None\n",
    "            else:\n",
    "                time.sleep(2 * attempt)  # simple backoff\n",
    "        except requests.HTTPError as exc:\n",
    "            print(f\"HTTP error {exc.response.status_code}: {exc}\")\n",
    "            payload = None\n",
    "            break\n",
    "\n",
    "    if payload is None:\n",
    "        # Skip updating state for this iteration\n",
    "        continue\n",
    "\n",
    "    responses.append(payload)\n",
    "\n",
    "    docs = payload.get(\"documents\", [])\n",
    "    if not docs:\n",
    "        print(\"No documents returned, stopping.\")\n",
    "        break\n",
    "\n",
    "    dates = [doc[\"datum\"] for doc in docs if \"datum\" in doc]\n",
    "    if not dates:\n",
    "        print(\"Documents missing 'datum', stopping.\")\n",
    "        break\n",
    "\n",
    "    end_date = min(dates)\n",
    "    ep = docs[0][\"wahlperiode\"]\n",
    "\n",
    "    if ep <= earliest_ep:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8758e21c-f11b-4b5b-9e6a-b5ed45550d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "for j in range(len(responses)):\n",
    "    for i in range(len(responses[j]['documents'])):\n",
    "        dates.append(responses[j]['documents'][i]['datum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751dcc9-42e8-4f87-82b9-fc5adba390f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'The dataset ranges from {min(dates)} to {max(dates)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21fb0b-3663-4c56-a6e6-0dbdadcb0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_doc = len(responses[0]['documents'])*len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f187a19-047e-424d-b3b9-1d7b2de1b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'The dataset contains {n_doc} documents '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78562c3a-c612-4bff-baf4-f24ce8b4d91c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(responses[1]['documents'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c31400-a05e-489a-9cc4-454834aa6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# had to export at least one document to test smth\n",
    "# with open(\"test_speech_output.txt\", \"w\") as text_file:\n",
    "#     text_file.write(responses[3]['documents'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f2030-7baa-43ec-a4db-c6daa30588e1",
   "metadata": {},
   "source": [
    "### Just to get a feeling for the amount of tokes: Splitting and cleaning to approximate length of an example document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f68469-1e74-4f38-b803-7f7c92139cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start of discussion\n",
    "start = responses[1]['documents'][0]['text'].index('Die Sitzung ist hiermit eröffnet.')\n",
    "#end of discussion\n",
    "#end = responses[1]['documents'][0]['text'].index('Einen schönen restlichen Tag noch!')\n",
    "\n",
    "#slice corpus\n",
    "core_speeches = responses[1]['documents'][0]['text'][start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ceb696-bcda-4b94-8628-bff68ac0275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(sentence):\n",
    "\n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "\n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "\n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize\n",
    "    stop_words = set(stopwords.words('german')) ## define stopwords\n",
    "\n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in stop_words\n",
    "    ]\n",
    "\n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\")\n",
    "        for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "\n",
    "    #cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f779e-6434-494b-a5f8-79f4f9f91ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(core_speeches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
